{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f19089",
   "metadata": {},
   "source": [
    "# Extracción de categorías del libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae46810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Defino la base del sitio principal\n",
    "BASE_URL = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Creo la URL inicial del sitio y uso BASE_URL + url_relativa para armar URLs completas de las categorías\n",
    "CATEGORIAS_URL = BASE_URL + \"index.html\" \n",
    "\n",
    "# Extraigo con el método get las categorias y con el \"parser\" obtengo la info relevante\n",
    "respuesta = requests.get(CATEGORIAS_URL)\n",
    "soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "\n",
    "# Extaigo los enlaces de categorias (hay uno para cada categoria dentro de 'ul.nav-list')\n",
    "categorias_links = soup.select(\"ul.nav-list ul li a\")\n",
    "\n",
    "categoria = {} # Creo un diccionario vacio para guardar categorias\n",
    "\n",
    "for link in categorias_links: \n",
    "    nombre = link.text.strip() # Obtengo y limpio el nombre de la categoria\n",
    "    url_relativa = link['href'] # Saco la URL relativa del href\n",
    "    url_completa = BASE_URL + url_relativa # Armo la URL completa\n",
    "    categoria[nombre] = url_completa # Guardo en el diccionario: nombre -> URL\n",
    "\n",
    "print(f\"Categorias encontradas: {len(categoria)}\\n\")\n",
    "\n",
    "# Recorre cada par del diccionario : (clave,valor) y con el .items devuelve la lista de pares\n",
    "for nombre, url in categoria.items():\n",
    "    print(f\"- {nombre}: {url}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303d37d",
   "metadata": {},
   "source": [
    "# Obtención de libros por categoría\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a06224c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # Para no hacer pedidos(requests) una tras otra, porque puede causar problemas\n",
    "\n",
    "\"\"\"Creo una función y le doy dos argumentos\"\"\"\n",
    "def obtener_libros_por_categoria(nombre_categoria,url_categoria):\n",
    "\n",
    "    \n",
    "    pagina_actual = url_categoria #Asigna la URL inicial de la categoría a una variable que iremos actualizando\n",
    "    libros = [] # Lista vacía en donde se va guardar un diccionario por cada libro\n",
    "\n",
    "    while True:\n",
    "        print(f\"Scrapeando {pagina_actual}...\")\n",
    "        respuesta = requests.get(pagina_actual)\n",
    "        soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "\n",
    "        libros_en_pagina = soup.select(\".product_pod\")\n",
    "\n",
    "        for libro in libros_en_pagina:\n",
    "            titulo = libro.h3.a[\"title\"] # Accede directamente a las etiquetas y entra directo a la etiqueta <a> y saca el atributo title desde el objeto libro \n",
    "            precio = libro.select_one(\".price_color\").text.strip() #Busca la primera etiqueta que tenga esa clase \".price_color\" accede al texto dentro de la etiqueta y le saca espacios si tiene\n",
    "            rating = libro.p[\"class\"][1] # Accede a la segunda clase del rating\n",
    "            libros.append({\n",
    "                \"categoria\" : nombre_categoria,\n",
    "                \"titulo\": titulo,\n",
    "                \"precio\" : precio,\n",
    "                \"rating\" : rating\n",
    "            })\n",
    "\n",
    "        next_button = soup.select_one(\"li.next a\") # Busca si hay un enlace a la próxima página. Si existe, sigue\n",
    "        # Construye la URL de la próxima página\n",
    "        if next_button:\n",
    "            next_url = next_button[\"href\"]\n",
    "            # Si pagina_actual = \".../index.html\" y next_url = \"page-2.html\"➡ Reemplaza index.html con page-2.html\n",
    "            if \"index.html\" in pagina_actual:\n",
    "                pagina_actual = pagina_actual.replace(\"index.html\", next_url )\n",
    "            else:\n",
    "                # Esta línea se ejecuta cuando hay más de una página en la categoría (ejemplo: page-2.html, page-3.html, etc.).\n",
    "                pagina_actual = \"/\".join(pagina_actual.split(\"/\")[:-1])+ \"/\" + next_url\n",
    "        else: \n",
    "            break\n",
    "\n",
    "        time.sleep(1) # Espera 1 segundo antes de pasar a la próxima página\n",
    "    return libros # Devuelve la lista de diccionarios con todos los libros encontrados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24a7be",
   "metadata": {},
   "source": [
    "# Visualización de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88f784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lista vacía para juntar todos los libros del sitio\n",
    "todos_los_libros = []\n",
    "# Recorre todas las categorías que ya tenemos en el diccionario\n",
    "for nombre,url in categoria.items():\n",
    "    print(f\"Scrapeando categoría: {nombre}...\")\n",
    "\n",
    "    libros_categoria = obtener_libros_por_categoria(nombre,url)\n",
    "\n",
    "    todos_los_libros.extend(libros_categoria)  # agregamos todos los libros a la lista general\n",
    "print(f\"\\nScraping completado. Total de libros encontrados: {len(todos_los_libros)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
